{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec 개선\n",
    "- 계산 자원 문제 : embedding 계층(입력층 계산 낭비 감소 효과), negative sampling 손실 함수(은닉층 이후 계산 낭비 감소 효과)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14],\n",
       "       [15, 16, 17],\n",
       "       [18, 19, 20]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding 계층 구현\n",
    "\n",
    "import numpy as np\n",
    "W = np.arange(21).reshape(7,3)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 7, 8])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  4,  5],\n",
       "       [ 0,  1,  2],\n",
       "       [ 9, 10, 11],\n",
       "       [ 0,  1,  2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.array([1,0,3,0]) #원하는 행 번호 입력\n",
    "W[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding 계층의 forward() 메서드 구현\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self,W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward() 메서드 구현\n",
    "\n",
    "def backward(self, dout):\n",
    "    dW, = self.grads\n",
    "    dW[...] = 0 #배열의 모든 요소를 0으로 초기화\n",
    "\n",
    "    for i, word_id in enumerate(self.idx):\n",
    "        dW[self.idx] += dout #중복된 인덱스는 기울기를 더함으로써 누적되도록 = 여러번 등장했다는 의미\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dot 연산(내적) - 정답에 해당하는 열벡터와 은닉층 뉴런의 내적\n",
    "\n",
    "class EmbeddingDot:\n",
    "    def __init__(self,W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.parmas\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#negative sampling(단어 빈도수를 기준으로 부정적 예를 샘플링, 긍/부정 모두 손실 구하여 합한 값 = 최종 손실)\n",
    "## 즉, 선택과 집중('모두'가 아닌 '일부'를 처리 = 효율적인 계산 = 다중분류를 이진분류처럼 취급⭐)\n",
    "\n",
    "# 확률 분포 샘플링\n",
    "\n",
    "import numpy as np\n",
    "np.random.choice(10) #무작위 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['you', 'say', 'goodbye','I','hello','.']\n",
    "np.random.choice(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 있음 :  ['.' 'goodbye' 'you' 'say' '.']\n",
      "중복 없음 :  ['I' 'goodbye' 'say' '.' 'hello']\n"
     ]
    }
   ],
   "source": [
    "print('중복 있음 : ', np.random.choice(words, size=5))\n",
    "print('중복 없음 : ', np.random.choice(words, size=5, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [0.5, 0.1, 0.05, 0.2, 0.05, 0.1]\n",
    "np.random.choice(words, p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64196878 0.33150408 0.02652714]\n"
     ]
    }
   ],
   "source": [
    "# 출현 확률이 낮은 단어를 버리지 않도록(0.75 제곱을 통해 확률이 낮은 단어의 확률을 상승시킬 수 있음)\n",
    "p = [0.7,0.29,0.01]\n",
    "new_p = np.power(p,0.75)\n",
    "new_p /=np.sum(new_p)\n",
    "print(new_p) #기존 p의 0.01에 비해 결과값에서 0.02로 더 상승함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4]\n",
      " [4 1]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "#UnigramSampler 클래스\n",
    "\n",
    "from ch04.negative_sampling_layer import UnigramSampler\n",
    "\n",
    "corpus = np.array([0,1,2,3,4,1,2,3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1,3,0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "print(negative_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative sampling 구현\n",
    "\n",
    "class negativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5) : \n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward 구현\n",
    "\n",
    "def forward(self, h, target):\n",
    "    batch_size = target.shape[0]\n",
    "    negative_sample = self.sampler.get_negative_sample(target)\n",
    "\n",
    "    # 긍정적 예 순전파\n",
    "    score = self.embed_dot_layers[0].forward(h, target)\n",
    "    correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "    loss = self.loss_layers[0].forward(score, correct_label) #긍정\n",
    "\n",
    "    # 부정적 예 순전파\n",
    "    negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "    for i in range(self.sample_size):\n",
    "        negative_target = negative_sample[:,i]\n",
    "        score = self.embed_dot_layers[1+i].forward(h, negative_target)\n",
    "        loss += self.loss_layers[1+i].forward(score, negative_label) #1이상부터 부정\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward 구현\n",
    "\n",
    "def backward(self, dout=1):\n",
    "    dh = 0 #입력 벡터에 대한 기울기를 저장하기 위한 변수 초기화\n",
    "    for l0,l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "        dscore = l0.backward(dout) #dout : 역전파 시에 해당 층으로부터 전달되는 미분값(기울기)\n",
    "        dh += l1.backward(dscore)\n",
    "    return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 모델 적용(CBOW)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import Embedding\n",
    "from ch04.negative_sampling_layer import NegativeSamplingLoss\n",
    "\n",
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        #가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V,H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V,H).astype('f')\n",
    "\n",
    "        #계층 생성\n",
    "        self.in_layers = []\n",
    "        for i in range(2*window_size):\n",
    "            layer = Embedding(W_in)\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "\n",
    "        #모든 가중치와 기울기를 배열에 모은다.\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [],[]\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        #인스턴스 변수에 단어의 분산 표현을 저장\n",
    "        self.word_vecs=W_in\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward, backward\n",
    "\n",
    "def forward(self, contexts, target):\n",
    "    h=0\n",
    "    for i, layer in enumerate(self.in_layers):\n",
    "        h += layer.forward(contexts[:,i])\n",
    "    h *= 1/len(self.in_layers) #각 레이어의 출력을 더한 후 평균을 계산(정규화) = 입력 레이어 각각의 출력에 대한 평균적인 특징을 고려하여 전체적인 학습을 진행\n",
    "    loss = self.ns_loss.forward(h, target)\n",
    "    return loss\n",
    "\n",
    "def backward(self, dout=1):\n",
    "    dout = self.ns_loss.backward(dout)\n",
    "    dout *= 1/len(self.in_layers)\n",
    "    for layer in self.in_layers:\n",
    "        layer.backward(dout)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW 모델 학습 코드\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common import config\n",
    "import pickle\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from ch04.cbow import CBOW\n",
    "from common.util import create_contexts_target, to_cpu, to_gpu\n",
    "from dataset import ptb\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "if config.GPU:\n",
    "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "\n",
    "# 모델 등 생성\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 학습\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "# 나중에 사용할 수 있도록필요한 데이터 저장\n",
    "word_vecs = model.word_vecs\n",
    "if config.GPU:\n",
    "    word_vecs = to_cpu(word_vecs)\n",
    "params = {}\n",
    "params['word_vesc'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'\n",
    "with open(pkl_file, 'wb') as f: \n",
    "    pickle.dump(params, f, -1) #dump : 파이썬 객체를 직렬화하여 파일에 저장 / -1: 가장 최신의 프로토콜 버전을 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " we: 0.6103515625\n",
      " someone: 0.59130859375\n",
      " i: 0.55419921875\n",
      " something: 0.48974609375\n",
      " anyone: 0.47314453125\n",
      "\n",
      "[query] year\n",
      " month: 0.71875\n",
      " week: 0.65234375\n",
      " spring: 0.62744140625\n",
      " summer: 0.6259765625\n",
      " decade: 0.603515625\n",
      "\n",
      "[query] car\n",
      " luxury: 0.497314453125\n",
      " arabia: 0.47802734375\n",
      " auto: 0.47119140625\n",
      " disk-drive: 0.450927734375\n",
      " travel: 0.4091796875\n",
      "\n",
      "[query] toyota\n",
      " ford: 0.55078125\n",
      " instrumentation: 0.509765625\n",
      " mazda: 0.49365234375\n",
      " bethlehem: 0.47509765625\n",
      " nissan: 0.474853515625\n"
     ]
    }
   ],
   "source": [
    "# CBOW 모델 평가\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import most_similar\n",
    "import pickle\n",
    "\n",
    "pkl_file = './ch04/cbow_params.pkl' #상위 코드는 cpu 로 오랜 시간이 걸리므로 pickle 파일로 확인\n",
    "\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    word_vecs = params['word_vecs']\n",
    "    word_to_id = params['word_to_id']\n",
    "    id_to_word = params['id_to_word']\n",
    "\n",
    "queries = ['you', 'year', 'car','toyota']\n",
    "\n",
    "for query in queries:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[analogy] king:man = queen:?\n",
      " woman: 5.16015625\n",
      " veto: 4.9296875\n",
      " ounce: 4.69140625\n",
      " earthquake: 4.6328125\n",
      " successor: 4.609375\n",
      "\n",
      "[analogy] take:took = go:?\n",
      " went: 4.55078125\n",
      " points: 4.25\n",
      " began: 4.09375\n",
      " comes: 3.98046875\n",
      " oct.: 3.90625\n",
      "\n",
      "[analogy] car:cars = child:?\n",
      " children: 5.21875\n",
      " average: 4.7265625\n",
      " yield: 4.20703125\n",
      " cattle: 4.1875\n",
      " priced: 4.1796875\n",
      "\n",
      "[analogy] good:better = bad:?\n",
      " more: 6.6484375\n",
      " less: 6.0625\n",
      " rather: 5.21875\n",
      " slower: 4.734375\n",
      " greater: 4.671875\n"
     ]
    }
   ],
   "source": [
    "# 벡터 연산으로 유추 문제 풀이\n",
    "\n",
    "from common.util import analogy\n",
    "\n",
    "analogy('king','man','queen', word_to_id, id_to_word, word_vecs)\n",
    "analogy('take','took','go', word_to_id, id_to_word, word_vecs)\n",
    "analogy('car','cars','child', word_to_id, id_to_word, word_vecs)\n",
    "analogy('good','better','bad', word_to_id, id_to_word, word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
